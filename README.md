# Terminal-Based TensorFlow Serving on Kubernetes

This project demonstrates how to deploy a TensorFlow model locally using Docker, TensorFlow Serving, and Kubernetes via Minikube. All components â€” Python script, Dockerfile, and Kubernetes manifests â€” are authored directly in the terminal using `nano`.

> ğŸ’¡ Use case: zero-cost AI inference workflow for MLOps experimentation and GitOps-ready infrastructure.

---

## ğŸ“ Project Structure

tf-k8s-serve/ â”œâ”€â”€ model/ â”‚ â””â”€â”€ 1/ â”‚ â”œâ”€â”€ saved_model.pb â”‚ â””â”€â”€ variables/ â”œâ”€â”€ export_model.py â”œâ”€â”€ Dockerfile â”œâ”€â”€ deployment.yaml â”œâ”€â”€ service.yaml


---

## âš™ï¸ Prerequisites

- Python 3.x with TensorFlow installed
- Docker (with local build access)
- Minikube (Kubernetes cluster)
- Terminal with `nano` editor

---

## ğŸ§ª Step 1: Export a TensorFlow Model

Use the included Python script:

```bash
python3 export_model.py
This will train a basic Keras model and export it to model/1/ in TensorFlow SavedModel format.

ğŸ³ Step 2: Build Docker Image
Build your serving image from the root directory:

bash
docker build -t tfmodel:latest .
Ensure that .dockerignore does not exclude the model/ directory during build.

â˜¸ï¸ Step 3: Deploy to Kubernetes
Start Minikube and point Docker to its daemon:

bash
minikube start
eval $(minikube docker-env)
Apply your Kubernetes manifests:

bash
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
Check that your pod is running:

bash
kubectl get pods
ğŸŒ Step 4: Access the Model Endpoint
Expose the service via Minikube:

bash
minikube service tfmodel-service
Or hit it directly:

bash
curl http://$(minikube ip):30007/v1/models/tfmodel
You should see model status output confirming it's served.

ğŸ§± Architecture Diagram
[ Keras Model Export (Python) ]
            â”‚
            â–¼
  [ Local SavedModel Format ]
            â”‚
            â–¼
    [ Docker Image Build ]
            â”‚
            â–¼
[ TensorFlow Serving Container ]
            â”‚
            â–¼
   [ Kubernetes Deployment ]
            â”‚
            â–¼
  [ NodePort Service â†’ curl ]
ğŸ“ Notes
Model directory must follow TensorFlow Servingâ€™s versioned format (model/1/saved_model.pb)

All files and configurations were created using nano in the terminal â€” no IDE used

Can serve as a local GitOps lab foundation or be extended into CI/CD-based MLOps
